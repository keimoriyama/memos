# kaggle で使うやつのメモ

## 目的

未知のデータセットである Private LB に対する性能を高めること

## tips

テーブルデータは pandas を使うとわかりやすい

`train.csv`が学習用データで、`test.csv`がテストデータ。

`NaN`は欠損値

欠損値をそのまま扱えるアルゴリズムもあるけど、だいたいは中央値や平均値で穴埋めする。

訓練データとテストデータを縦に結合することで共通する処理を省略したり、テストデータの情報も考慮して処理を実行できる

実務では良くないという意見も合ったりなかったり

再現性が無いとモデルが改善されたかの評価が難しい

対応としては、乱数を用いるのではなく固定のシード値を用いる

コンペのテーマに関するドメイン知識（専門的な知識）も有効だったりする

## 特徴量エンジニアリング

### 大切な事

予測に寄与する新しい特徴量を作るためには、仮設を立てて、データを可視化することを繰り返すことが重要

### 特徴量エンジニアリングとは

- 読み込んだデータを機械学習アルゴリズムが扱えるように変換すること

- もともとあるデータから有用な新しいデータ（特徴量）を作ること

### データの標準化

機械学習アルゴリズムの中には、データの取る値が異なると学習がうまく行かない場合がある。

なので平均０、標準偏差１に標準化することでこの問題を解決する

近年ではこの特徴量の変換の影響を受けづらいアルゴリズムが採用されることが多い

### 欠損値の値の扱い

- 欠損値としてそのまま扱う

- 代表的な値（平均値や中央値）で欠損値を補完する

- 他の特徴料から欠損値を予測して補完する

- 欠損値か否かの情報をもとに新しい特徴量を作る

### 探索的データ分析

これで差が出ることもおおい

1. Pandas Profiling で概要の確認

2. 各特徴量について目的変数との関係を確認する

### ハイパーパラメータの調整

Disucussion などで公開されているものもあるので、それを流用するのもあり

1. 手動

2. チューニングツールを使う

ハイパーパラメータを調整するためには、ハイパーパラメータを正しく理解する必要がある。（公式ドキュメントを参照する）

## validation について

submit したときのスコアで確認するのは、回数制限や一部のデータに対する過学習などの問題点がある。

なので、学習用データセットを使って検証用データセットを作り、モデルの性能を評価するのが一般的。

### Cross Validation

訓練データを使って異なる方法でデータセットでデータを分割して、モデルをトレーニングする。

## コンペの選び方

1. メダルの有無

メダルがあるコンペほど参加者の質が高く、人数も多くなるので学びが多い

2. コンペで扱うデータ

データは大きく分けて 3 種類

- テーブルデータ
- 画像データ
- テキストデータ

3. 開催期間

コンペの開催期間は通常 2〜3 ヶ月。

初参加の場合には期間が数週間〜1 ヶ月程度が情報などが Discussion や Notebook に集まっているのでオススメ

## 初心者の戦い方

1. 概要やルールの確認

解決したい課題への理解は必須。コンペの目的などを確認や評価指標を理解しておくこと。

2. データの確認

参加者の Notebook や自分で確認してデータを様々な角度から見る。

Notebook は Most Votes でソートして見ると良い

3. ベンチマークの作成

どのようにデータセットを分割して検証用のデータを作成するのかや、機械学習アルゴリズムを用いるのかを考える。

公開されている Notebook を参考にすると良い。

Best Score でソートして参考にする。なるべくシンプルなものを選んでおいて、後で自分で改良していく。

4. ベンチマークの改善

特徴量などを追加してベンチマークを改善する。

スコアは改善しないこともあるけど、気長に試行錯誤をすることが大事。

- データサイズが大きいときには特に、特徴量を何度も作り直したりせず、一度作った特徴量は保存しておいて後から使えるようにする
- 複数回実行する処理はモジュール化する
- 実験の結果を記録しておいて、後から見返せるようにする
- 過去のコンペで使ったソースを整理する

5. アンサンブルなどでスコアの上積みを狙う

複数の予想を作り、単純平均するだけでもある程度効果はある。
